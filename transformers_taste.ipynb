{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093a7f98",
   "metadata": {},
   "source": [
    "# A taste of transformers\n",
    "\n",
    "## Device setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa3b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Select best available device\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5ec96",
   "metadata": {},
   "source": [
    "## Model properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9ff772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: gpt2\n",
      "Hidden size: 768\n",
      "Number of attention heads: 12\n",
      "Number of layers: 12\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# Load pretrained GPT-2 using Hugging Face Transformers\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Add padding token for consistency\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Some properties of the model\n",
    "print(f\"Model type: {config.model_type}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bee76",
   "metadata": {},
   "source": [
    "## Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a70e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example vocabulary: ['Ġmind', 'aff', 'omm', 'Ġfuture', 'ged', 'Ġcut', 'Ġtot', 'itch', 'Ġvideo', 'Ġinvestig']\n"
     ]
    }
   ],
   "source": [
    "# Explore the tokenizer's vocabulary\n",
    "sorted_vocab = sorted(list(tokenizer.vocab.items()), key=lambda n: n[1])\n",
    "example_vocab = [vocab[0] for vocab in sorted_vocab[2000:2010]]\n",
    "print(f\"Example vocabulary: {example_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4cf5a",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c272ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: London is the capital of the United\n",
      "Tokenized input: tensor([[23421,   318,   262,  3139,   286,   262,  1578]], device='mps:0')\n",
      "Input tokens as strings: ['London', 'Ġis', 'Ġthe', 'Ġcapital', 'Ġof', 'Ġthe', 'ĠUnited']\n"
     ]
    }
   ],
   "source": [
    "# Text to analyze\n",
    "reference_text = \"London is the capital of the United\"\n",
    "\n",
    "# Convert text to tokens\n",
    "tokens = tokenizer.encode(reference_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Show tokenization results\n",
    "print(f\"Input text: {reference_text}\")\n",
    "print(f\"Tokenized input: {tokens}\")\n",
    "print(f\"Input tokens as strings: {tokenizer.convert_ids_to_tokens(tokens[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1937d04b",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b915defd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most likely next tokens:\n",
      " 1. ' Kingdom' (prob: 0.7349)\n",
      " 2. ' States' (prob: 0.2262)\n",
      " 3. ' Arab' (prob: 0.0195)\n",
      " 4. ' Nations' (prob: 0.0107)\n",
      " 5. ' State' (prob: 0.0009)\n",
      "Continuing with most likely tokens:\n",
      " 1: 'London is the capital of the United Kingdom'\n",
      " 2: 'London is the capital of the United States'\n",
      " 3: 'London is the capital of the United Arab'\n",
      " 4: 'London is the capital of the United Nations'\n",
      " 5: 'London is the capital of the United State'\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Run model and get outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Manual language modeling head - linear projection to vocabulary size\n",
    "    # Get the embedding weights to use as the language modeling head (weight tying)\n",
    "    embed_weights = model.wte.weight  # [vocab_size, hidden_size]\n",
    "\n",
    "    # Apply language modeling head: hidden_states @ embed_weights.T\n",
    "    logits = torch.matmul(\n",
    "        last_hidden_states, embed_weights.T\n",
    "    )  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "# Get probabilities for next token from last position\n",
    "next_token_probs = softmax(logits[0, -1], dim=-1)\n",
    "\n",
    "# Get top 10 most likely next tokens\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "print(\"\\nTop 10 most likely next tokens:\")\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    token_str = tokenizer.decode(idx)\n",
    "    print(f\"{i + 1:2d}. '{token_str}' (prob: {prob:.4f})\")\n",
    "\n",
    "# Show what text would look like with each top prediction\n",
    "print(\"Continuing with most likely tokens:\")\n",
    "for i in range(top_k):\n",
    "    print(f\"{i + 1:2d}: '{reference_text + tokenizer.decode(top_indices[i])}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
